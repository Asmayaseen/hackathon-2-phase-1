"""
Chat endpoint with OpenAI Agents SDK integration.

Tasks: T-402 to T-406 - Implement Chat Endpoint
Spec: specs-history/phase-3-chatbot/spec.md Â§3.1.4, Â§3.1.5
Plan: specs-history/phase-3-chatbot/plan.md Phase 4

This module provides a stateless chat endpoint that:
1. Manages conversation persistence in database
2. Integrates with OpenAI Agents SDK
3. Uses MCP tools for task operations
4. Maintains conversation context
5. Provides natural language interface

Following Constitution principles:
- Stateless Architecture: No in-memory state
- MCP-First: All task ops through MCP tools
- User Authorization: JWT verification + user_id validation
- Error Handling: Graceful, user-friendly messages
"""

from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from sqlmodel import Session, select
from typing import Optional, List, Dict, Any, AsyncIterator
import os
import logging
import json
import asyncio

from database import get_db
from models import Conversation, Message
from mcp_server.tools import add_task, list_tasks, complete_task, delete_task, update_task

# OpenAI SDK (will be imported when available)
# from openai import OpenAI

logger = logging.getLogger(__name__)

# Router configuration
router = APIRouter(prefix="/api/{user_id}/chat", tags=["chat"])


# Request/Response Schemas
class ChatRequest(BaseModel):
    """Chat request schema."""
    conversation_id: Optional[int] = Field(None, description="Existing conversation ID (creates new if not provided)")
    message: str = Field(..., min_length=1, max_length=2000, description="User's message")


class ToolCall(BaseModel):
    """Tool call information."""
    tool: str = Field(..., description="Name of the tool called")
    parameters: Dict[str, Any] = Field(default_factory=dict, description="Parameters passed to the tool")


class ChatResponse(BaseModel):
    """Chat response schema."""
    conversation_id: int = Field(..., description="Conversation ID")
    response: str = Field(..., description="AI assistant's response")
    tool_calls: List[ToolCall] = Field(default_factory=list, description="Tools invoked during processing")


# Conversation Management Functions
async def get_or_create_conversation(
    user_id: str,
    conversation_id: Optional[int],
    db: Session
) -> int:
    """
    Get existing conversation or create new one.

    Task: T-403 - Implement Conversation Management
    Constitution: Database Principles - Conversation Persistence

    Args:
        user_id: User ID who owns the conversation
        conversation_id: Existing conversation ID (optional)
        db: Database session

    Returns:
        int: Conversation ID
    """
    if conversation_id:
        # Check if conversation exists and belongs to user
        statement = select(Conversation).where(
            Conversation.id == conversation_id,
            Conversation.user_id == user_id
        )
        conv = db.exec(statement).first()

        if conv:
            logger.info(f"Using existing conversation: user={user_id}, conv_id={conversation_id}")
            return conv.id

    # Create new conversation
    conv = Conversation(user_id=user_id)
    db.add(conv)
    db.commit()
    db.refresh(conv)

    logger.info(f"Created new conversation: user={user_id}, conv_id={conv.id}")
    return conv.id


async def load_conversation_history(
    conversation_id: int,
    limit: int = 10,
    db: Session = None
) -> List[Dict[str, str]]:
    """
    Load last N messages from conversation.

    Task: T-403 - Implement Conversation Management
    Constitution: Performance Principles - Minimize Database Queries

    Args:
        conversation_id: Conversation ID
        limit: Number of messages to load (default: 10)
        db: Database session

    Returns:
        list: Messages in OpenAI format [{"role": "user"|"assistant", "content": "..."}]

    Note:
        We limit to 10 messages to:
        1. Reduce OpenAI token costs
        2. Keep context window manageable
        3. Improve response speed
    """
    # Load messages ordered by creation time (most recent first)
    statement = select(Message).where(
        Message.conversation_id == conversation_id
    ).order_by(Message.created_at.desc()).limit(limit)

    messages = db.exec(statement).all()

    # Reverse to chronological order (oldest first)
    messages = list(reversed(messages))

    logger.info(f"Loaded {len(messages)} messages for conversation {conversation_id}")

    # Convert to OpenAI format
    return [
        {"role": msg.role, "content": msg.content}
        for msg in messages
    ]


async def save_message(
    conversation_id: int,
    user_id: str,
    role: str,
    content: str,
    db: Session
) -> None:
    """
    Save a message to the database.

    Task: T-403 - Implement Conversation Management
    Constitution: Database Principles - Conversation Persistence

    Args:
        conversation_id: Conversation ID
        user_id: User ID
        role: Message role ("user" or "assistant")
        content: Message content
        db: Database session
    """
    msg = Message(
        conversation_id=conversation_id,
        user_id=user_id,
        role=role,
        content=content
    )
    db.add(msg)
    db.commit()

    logger.debug(f"Saved message: conv={conversation_id}, role={role}, length={len(content)}")


async def get_ai_response_stream(
    messages: List[Dict[str, str]],
    user_id: str
) -> AsyncIterator[str]:
    """
    Stream AI response using OpenAI API with function calling.

    Task: SSE Streaming Implementation
    Constitution: Real-time Response Streaming

    Args:
        messages: Conversation history in OpenAI format
        user_id: User ID (for MCP tool calls)

    Yields:
        str: SSE-formatted chunks of the response
    """
    try:
        from openai import OpenAI
        import json as json_module

        # Check if OpenAI API key is configured
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not set, using mock streaming response")
            async for chunk in get_mock_ai_response_stream(messages, user_id):
                yield chunk
            return

        client = OpenAI(api_key=api_key)

        # Define available MCP tools for OpenAI
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "add_task",
                    "description": "Create a new task in the user's todo list",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string",
                                "description": "The task title or description"
                            },
                            "description": {
                                "type": "string",
                                "description": "Optional detailed description"
                            }
                        },
                        "required": ["title"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "list_tasks",
                    "description": "Get the user's tasks, optionally filtered by status",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "status": {
                                "type": "string",
                                "enum": ["all", "pending", "completed"],
                                "description": "Filter tasks by status (default: all)"
                            }
                        }
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "complete_task",
                    "description": "Toggle the completion status of a task",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task_id": {
                                "type": "integer",
                                "description": "The ID of the task to toggle"
                            }
                        },
                        "required": ["task_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "delete_task",
                    "description": "Delete a task from the user's todo list",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task_id": {
                                "type": "integer",
                                "description": "The ID of the task to delete"
                            }
                        },
                        "required": ["task_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "update_task",
                    "description": "Update a task's title or description",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task_id": {
                                "type": "integer",
                                "description": "The ID of the task to update"
                            },
                            "title": {
                                "type": "string",
                                "description": "New title for the task"
                            },
                            "description": {
                                "type": "string",
                                "description": "New description for the task"
                            }
                        },
                        "required": ["task_id"]
                    }
                }
            }
        ]

        # Add system message for AI personality
        system_message = {
            "role": "system",
            "content": """You are a helpful AI assistant for managing todo tasks. You have access to tools to help users:
- Create new tasks
- View their tasks (all, pending, or completed)
- Mark tasks as complete/incomplete
- Update task details
- Delete tasks

Be friendly, concise, and helpful. When users ask to add tasks, extract the task details and use the add_task function.
When showing tasks, format them clearly with their IDs. Always confirm actions taken."""
        }

        # Call OpenAI API with streaming enabled
        stream = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[system_message] + messages,
            tools=tools,
            tool_choice="auto",
            temperature=0.7,
            max_tokens=500,
            stream=True
        )

        full_response = ""
        tool_calls_buffer = {}

        # Process streaming response
        for chunk in stream:
            delta = chunk.choices[0].delta

            # Handle content chunks
            if delta.content:
                full_response += delta.content
                # Send SSE event with content chunk
                yield f"data: {json_module.dumps({'type': 'content', 'content': delta.content})}\n\n"
                await asyncio.sleep(0)  # Allow other tasks to run

            # Handle tool calls
            if delta.tool_calls:
                for tool_call in delta.tool_calls:
                    idx = tool_call.index
                    if idx not in tool_calls_buffer:
                        tool_calls_buffer[idx] = {
                            "id": tool_call.id or "",
                            "name": "",
                            "arguments": ""
                        }

                    if tool_call.function:
                        if tool_call.function.name:
                            tool_calls_buffer[idx]["name"] = tool_call.function.name
                        if tool_call.function.arguments:
                            tool_calls_buffer[idx]["arguments"] += tool_call.function.arguments

        # Execute tool calls if any
        if tool_calls_buffer:
            for tool_call_data in tool_calls_buffer.values():
                function_name = tool_call_data["name"]
                arguments = json_module.loads(tool_call_data["arguments"])

                logger.info(f"AI calling tool: {function_name} with args: {arguments}")

                # Send tool call notification
                yield f"data: {json_module.dumps({'type': 'tool_call', 'tool': function_name, 'parameters': arguments})}\n\n"

                # Execute tool
                result = None
                if function_name == "add_task":
                    result = add_task(user_id, arguments.get("title"), arguments.get("description"))
                    # Generate confirmation message
                    if result and "title" in result:
                        confirmation = f"âœ… I've added '{result['title']}' to your tasks!"
                        full_response += confirmation
                        yield f"data: {json_module.dumps({'type': 'content', 'content': confirmation})}\n\n"
                elif function_name == "list_tasks":
                    status = arguments.get("status", "all")
                    result = list_tasks(user_id, status)
                    # Generate task list message
                    if isinstance(result, list):
                        if len(result) > 0:
                            task_list = "\n".join([f"â€¢ {t.get('title', 'Untitled')} {'âœ…' if t.get('completed') else 'â­•'}" for t in result[:10]])
                            confirmation = f"Here are your tasks:\n{task_list}"
                        else:
                            confirmation = "You don't have any tasks yet!"
                        full_response += confirmation
                        yield f"data: {json_module.dumps({'type': 'content', 'content': confirmation})}\n\n"
                elif function_name == "complete_task":
                    result = complete_task(user_id, arguments["task_id"])
                    if result and "title" in result:
                        status = "completed" if result.get("completed") else "incomplete"
                        confirmation = f"âœ… Marked '{result['title']}' as {status}!"
                        full_response += confirmation
                        yield f"data: {json_module.dumps({'type': 'content', 'content': confirmation})}\n\n"
                elif function_name == "delete_task":
                    result = delete_task(user_id, arguments["task_id"])
                    if result and "title" in result:
                        confirmation = f"ðŸ—‘ï¸ Deleted '{result['title']}'!"
                        full_response += confirmation
                        yield f"data: {json_module.dumps({'type': 'content', 'content': confirmation})}\n\n"
                elif function_name == "update_task":
                    result = update_task(
                        user_id,
                        arguments["task_id"],
                        arguments.get("title"),
                        arguments.get("description")
                    )
                    if result and "title" in result:
                        confirmation = f"âœï¸ Updated '{result['title']}'!"
                        full_response += confirmation
                        yield f"data: {json_module.dumps({'type': 'content', 'content': confirmation})}\n\n"

                # Send tool result
                if result:
                    yield f"data: {json_module.dumps({'type': 'tool_result', 'tool': function_name, 'result': result})}\n\n"

        # Send completion event
        yield f"data: {json_module.dumps({'type': 'done', 'full_response': full_response})}\n\n"

    except ImportError:
        logger.warning("OpenAI library not available, using mock streaming response")
        async for chunk in get_mock_ai_response_stream(messages, user_id):
            yield chunk
    except Exception as e:
        logger.error(f"OpenAI streaming error: {str(e)}")
        yield f"data: {json.dumps({'type': 'error', 'message': 'Error processing request'})}\n\n"


async def get_mock_ai_response_stream(
    messages: List[Dict[str, str]],
    user_id: str
) -> AsyncIterator[str]:
    """
    Mock streaming AI response for testing without OpenAI API.
    """
    last_message = messages[-1]["content"].lower() if messages else ""

    # Simulate streaming by sending response word by word
    response = "I can help you manage your tasks!"

    if "add" in last_message or "create" in last_message:
        response = "I'll add that task for you right away!"
    elif "show" in last_message or "list" in last_message:
        response = "Let me fetch your tasks for you..."

    # Stream response word by word
    words = response.split()
    for word in words:
        yield f"data: {json.dumps({'type': 'content', 'content': word + ' '})}\n\n"
        await asyncio.sleep(0.05)  # Simulate network delay

    yield f"data: {json.dumps({'type': 'done', 'full_response': response})}\n\n"


async def get_ai_response(
    messages: List[Dict[str, str]],
    user_id: str
) -> tuple[str, List[ToolCall]]:
    """
    Get AI response using OpenAI API with function calling (non-streaming version).

    Task: T-404 - Integrate OpenAI Agents SDK
    Constitution: AI Agent Principles

    Args:
        messages: Conversation history in OpenAI format
        user_id: User ID (for MCP tool calls)

    Returns:
        tuple: (response_text, tool_calls_made)
    """
    try:
        from openai import OpenAI
        import json

        # Check if OpenAI API key is configured
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not set, using mock response")
            return await get_mock_ai_response(messages, user_id)

        client = OpenAI(api_key=api_key)
        tool_calls_made = []

        # Define available MCP tools for OpenAI
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "add_task",
                    "description": "Create a new task in the user's todo list",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "title": {
                                "type": "string",
                                "description": "The task title or description"
                            },
                            "description": {
                                "type": "string",
                                "description": "Optional detailed description"
                            }
                        },
                        "required": ["title"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "list_tasks",
                    "description": "Get the user's tasks, optionally filtered by status",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "status": {
                                "type": "string",
                                "enum": ["all", "pending", "completed"],
                                "description": "Filter tasks by status (default: all)"
                            }
                        }
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "complete_task",
                    "description": "Toggle the completion status of a task",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task_id": {
                                "type": "integer",
                                "description": "The ID of the task to toggle"
                            }
                        },
                        "required": ["task_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "delete_task",
                    "description": "Delete a task from the user's todo list",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task_id": {
                                "type": "integer",
                                "description": "The ID of the task to delete"
                            }
                        },
                        "required": ["task_id"]
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "update_task",
                    "description": "Update a task's title or description",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "task_id": {
                                "type": "integer",
                                "description": "The ID of the task to update"
                            },
                            "title": {
                                "type": "string",
                                "description": "New title for the task"
                            },
                            "description": {
                                "type": "string",
                                "description": "New description for the task"
                            }
                        },
                        "required": ["task_id"]
                    }
                }
            }
        ]

        # Add system message for AI personality
        system_message = {
            "role": "system",
            "content": """You are a helpful AI assistant for managing todo tasks. You have access to tools to help users:
- Create new tasks
- View their tasks (all, pending, or completed)
- Mark tasks as complete/incomplete
- Update task details
- Delete tasks

Be friendly, concise, and helpful. When users ask to add tasks, extract the task details and use the add_task function.
When showing tasks, format them clearly with their IDs. Always confirm actions taken."""
        }

        # Call OpenAI API with function calling
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Using cost-effective model
            messages=[system_message] + messages,
            tools=tools,
            tool_choice="auto",
            temperature=0.7,
            max_tokens=500
        )

        message = response.choices[0].message

        # Check if AI wants to call functions
        if message.tool_calls:
            # Execute each tool call
            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                arguments = json.loads(tool_call.function.arguments)

                logger.info(f"AI calling tool: {function_name} with args: {arguments}")

                # Call the appropriate MCP tool
                if function_name == "add_task":
                    add_task(user_id, arguments.get("title"), arguments.get("description"))
                    tool_calls_made.append(ToolCall(tool="add_task", parameters=arguments))

                elif function_name == "list_tasks":
                    status = arguments.get("status", "all")
                    list_tasks(user_id, status)
                    tool_calls_made.append(ToolCall(tool="list_tasks", parameters={"status": status}))

                elif function_name == "complete_task":
                    complete_task(user_id, arguments["task_id"])
                    tool_calls_made.append(ToolCall(tool="complete_task", parameters=arguments))

                elif function_name == "delete_task":
                    delete_task(user_id, arguments["task_id"])
                    tool_calls_made.append(ToolCall(tool="delete_task", parameters=arguments))

                elif function_name == "update_task":
                    update_task(
                        user_id,
                        arguments["task_id"],
                        arguments.get("title"),
                        arguments.get("description")
                    )
                    tool_calls_made.append(ToolCall(tool="update_task", parameters=arguments))

            # Get final response from AI after tool execution
            # In a full implementation, we'd send tool results back to AI
            # For now, use the AI's initial message or generate a confirmation
            if message.content:
                final_response = message.content
            else:
                # Generate a simple confirmation based on tools called
                action = tool_calls_made[0].tool if tool_calls_made else "action"
                final_response = f"Done! I've completed your {action.replace('_', ' ')} request."
        else:
            # AI responded without calling tools
            final_response = message.content or "I'm here to help with your tasks!"

        logger.info(f"AI response: {final_response}, tools called: {len(tool_calls_made)}")
        return final_response, tool_calls_made

    except ImportError:
        logger.warning("OpenAI library not available, using mock response")
        return await get_mock_ai_response(messages, user_id)
    except Exception as e:
        logger.error(f"OpenAI API error: {str(e)}")
        return await get_mock_ai_response(messages, user_id)


# Mock AI Response (fallback when OpenAI is not configured)
async def get_mock_ai_response(
    messages: List[Dict[str, str]],
    user_id: str
) -> tuple[str, List[ToolCall]]:
    """
    Fallback mock AI response when OpenAI API is not available.
    Uses simple pattern matching.
    """
    last_message = messages[-1]["content"].lower() if messages else ""
    tool_calls = []

    # Simple pattern matching for demo
    if "add" in last_message or "create" in last_message:
        task_title = last_message.replace("add", "").replace("create", "").replace("task", "").strip()
        if task_title:
            result = add_task(user_id, task_title)
            tool_calls.append(ToolCall(tool="add_task", parameters={"title": task_title}))
            if "error" not in result:
                response = f"âœ“ Added '{result['title']}' to your todo list!"
            else:
                response = f"Sorry, couldn't add task: {result['error']}"
        else:
            response = "What task would you like to add?"

    elif "show" in last_message or "list" in last_message or "what" in last_message:
        result = list_tasks(user_id, "all")
        tool_calls.append(ToolCall(tool="list_tasks", parameters={"status": "all"}))
        if isinstance(result, list) and len(result) > 0:
            task_list = "\n".join([f"#{t['id']} - {t['title']} {'âœ“' if t['completed'] else 'â—‹'}" for t in result[:10]])
            response = f"Your tasks:\n{task_list}"
        else:
            response = "You don't have any tasks yet!"

    elif "complete" in last_message or "done" in last_message or "finish" in last_message:
        words = last_message.split()
        task_id = None
        for word in words:
            if word.isdigit():
                task_id = int(word)
                break
        if task_id:
            result = complete_task(user_id, task_id)
            tool_calls.append(ToolCall(tool="complete_task", parameters={"task_id": task_id}))
            if "error" not in result:
                response = f"âœ“ Marked '{result['title']}' as {result['status']}!"
            else:
                response = f"Couldn't complete task: {result['error']}"
        else:
            response = "Which task number would you like to complete?"

    elif "delete" in last_message or "remove" in last_message:
        words = last_message.split()
        task_id = None
        for word in words:
            if word.isdigit():
                task_id = int(word)
                break
        if task_id:
            result = delete_task(user_id, task_id)
            tool_calls.append(ToolCall(tool="delete_task", parameters={"task_id": task_id}))
            if "error" not in result:
                response = f"âœ“ Deleted '{result['title']}'!"
            else:
                response = f"Couldn't delete task: {result['error']}"
        else:
            response = "Which task number would you like to delete?"

    else:
        response = "I can help you manage tasks! Try:\nâ€¢ 'add buy milk'\nâ€¢ 'show my tasks'\nâ€¢ 'complete task 1'\nâ€¢ 'delete task 2'"

    return response, tool_calls


# Main Chat Endpoint
@router.post("", response_model=ChatResponse)
async def chat(
    user_id: str,
    request: ChatRequest,
    db: Session = Depends(get_db)
):
    """
    Stateless chat endpoint with OpenAI Agents SDK.

    TEMPORARILY BYPASSED AUTH FOR TESTING
    TODO: Re-enable JWT verification once Better Auth is configured

    Tasks: T-402 to T-406 - Complete Chat Endpoint
    Constitution: Stateless Architecture, MCP-First, User Authorization

    Args:
        user_id: User ID from URL path
        request: Chat request with message and optional conversation_id
        db: Database session

    Returns:
        ChatResponse: AI response with conversation_id and tool calls

    Raises:
        HTTPException: 500 if server error occurs

    Flow:
        1. (Auth check bypassed for testing)
        2. Get or create conversation
        3. Load conversation history
        4. Save user message
        5. Get AI response (with MCP tools)
        6. Save assistant response
        7. Return response
        8. Server forgets everything (stateless!)
    """
    # Step 1: Auth check temporarily bypassed
    # TODO: Re-enable this when Better Auth is configured:
    # if token.get("user_id") != user_id:
    #     logger.warning(f"Authorization failed: token user={token.get('user_id')}, requested user={user_id}")
    #     raise HTTPException(status_code=403, detail="Unauthorized")

    try:
        # Step 2: Get or create conversation
        conv_id = await get_or_create_conversation(
            user_id=user_id,
            conversation_id=request.conversation_id,
            db=db
        )

        # Step 3: Load conversation history
        history = await load_conversation_history(conv_id, limit=10, db=db)

        # Step 4: Save user message
        await save_message(
            conversation_id=conv_id,
            user_id=user_id,
            role="user",
            content=request.message,
            db=db
        )

        # Step 5: Build messages for AI agent
        messages = history + [{"role": "user", "content": request.message}]

        # Step 6: Get AI response (with MCP tools)
        assistant_message, tool_calls = await get_ai_response(messages, user_id)

        # Step 7: Save assistant response
        await save_message(
            conversation_id=conv_id,
            user_id=user_id,
            role="assistant",
            content=assistant_message,
            db=db
        )

        logger.info(f"Chat completed: user={user_id}, conv={conv_id}, tools={len(tool_calls)}")

        # Step 8: Return response (server now forgets everything - stateless!)
        return ChatResponse(
            conversation_id=conv_id,
            response=assistant_message,
            tool_calls=tool_calls
        )

    except HTTPException:
        # Re-raise HTTP exceptions
        raise

    except Exception as e:
        # Log error and return user-friendly message
        logger.error(f"Chat endpoint error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="I'm having trouble processing that. Please try again."
        )


# SSE Streaming Chat Endpoint
@router.post("/stream")
async def chat_stream(
    user_id: str,
    request: ChatRequest,
    db: Session = Depends(get_db)
):
    """
    Server-Sent Events (SSE) streaming chat endpoint.

    Constitution: Real-time Response Streaming (Section 6.2)

    Args:
        user_id: User ID from URL path
        request: Chat request with message and optional conversation_id
        db: Database session

    Returns:
        StreamingResponse: SSE stream of AI response chunks

    SSE Event Types:
        - content: Partial response text chunk
        - tool_call: Tool invocation notification
        - tool_result: Tool execution result
        - done: Streaming complete with full response
        - error: Error occurred during processing
    """
    async def event_generator():
        try:
            # Get or create conversation
            conv_id = await get_or_create_conversation(
                user_id=user_id,
                conversation_id=request.conversation_id,
                db=db
            )

            # Send conversation ID immediately
            yield f"data: {json.dumps({'type': 'conversation_id', 'conversation_id': conv_id})}\n\n"

            # Load conversation history
            history = await load_conversation_history(conv_id, limit=10, db=db)

            # Save user message
            await save_message(
                conversation_id=conv_id,
                user_id=user_id,
                role="user",
                content=request.message,
                db=db
            )

            # Build messages for AI
            messages = history + [{"role": "user", "content": request.message}]

            # Stream AI response
            full_response = ""
            async for chunk in get_ai_response_stream(messages, user_id):
                yield chunk

                # Extract full response from done event
                if '"type": "done"' in chunk:
                    import json as json_module
                    chunk_data = json_module.loads(chunk.replace("data: ", "").strip())
                    full_response = chunk_data.get("full_response", "")

            # Save assistant response
            if full_response:
                await save_message(
                    conversation_id=conv_id,
                    user_id=user_id,
                    role="assistant",
                    content=full_response,
                    db=db
                )

            logger.info(f"Stream completed: user={user_id}, conv={conv_id}")

        except Exception as e:
            logger.error(f"Stream error: {str(e)}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'message': 'Stream interrupted'})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable buffering in nginx
        }
    )


# Health check for chat endpoint
@router.get("/health")
async def chat_health():
    """Chat endpoint health check."""
    return {
        "status": "healthy",
        "endpoint": "chat",
        "mcp_tools": ["add_task", "list_tasks", "complete_task", "delete_task", "update_task"],
        "streaming": True
    }
